from playwright.sync_api import sync_playwright
from pathlib import Path
import json, re, requests, time
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime, timedelta
import random

AUTHOR = "patrick-lagace"
OUTPUT_DIR = Path("~/Desktop/chroniques_lagace_complet").expanduser()
OUTPUT_DIR.mkdir(exist_ok=True)

# Set pour √©viter les doublons
unique_articles = set()
scraped_urls = set()

def clean_filename(text):
    """Nettoie le texte pour cr√©er un nom de fichier valide"""
    cleaned = re.sub(r'[^\w\s-]', '_', text)
    cleaned = re.sub(r'\s+', '_', cleaned)
    return cleaned[:60].strip('_')

def extract_article_content(soup):
    """Extrait le contenu principal de l'article"""
    content_selectors = [
        "article p",
        ".article-content p",
        ".story-content p",
        ".content p",
        "div[data-module='ArticleBody'] p",
        ".text-content p",
        ".entry-content p",
        ".post-content p",
        ".article-body p"
    ]
    
    texte = ""
    for selector in content_selectors:
        paragraphs = soup.select(selector)
        if paragraphs:
            texte = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
            break
    
    return texte

def is_lagace_article(soup, url):
    """V√©rifie si l'article est bien de Patrick Lagac√©"""
    # V√©rifier dans l'URL
    if "patrick-lagace" in url.lower() or "lagace" in url.lower():
        return True
    
    # V√©rifier dans les m√©tadonn√©es auteur
    author_selectors = [
        ".author",
        ".byline",
        "[data-testid='author']",
        ".article-author",
        ".story-author",
        "span[class*='author']",
        "div[class*='author']",
        "meta[name='author']"
    ]
    
    for selector in author_selectors:
        author_elem = soup.select_one(selector)
        if author_elem:
            author_text = author_elem.get_text(strip=True) if hasattr(author_elem, 'get_text') else author_elem.get('content', '')
            if "patrick" in author_text.lower() and "lagac" in author_text.lower():
                return True
    
    # V√©rifier dans le JSON-LD
    json_ld_scripts = soup.find_all("script", type="application/ld+json")
    for script in json_ld_scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, dict) and "author" in data:
                author_name = data["author"]
                if isinstance(author_name, dict):
                    author_name = author_name.get("name", "")
                if "patrick" in str(author_name).lower() and "lagac" in str(author_name).lower():
                    return True
        except:
            pass
    
    return False

def scrape_archives():
    """Scrape les archives de mani√®re syst√©matique"""
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        
        # Configuration des headers
        page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # 1. Page auteur principale - avec pagination syst√©matique
        print("üîç Scraping page auteur avec pagination...")
        base_url = f"https://www.lapresse.ca/auteurs/{AUTHOR}"
        
        page_num = 1
        while page_num <= 50:  # Limiter √† 50 pages max
            try:
                if page_num == 1:
                    url = base_url
                else:
                    url = f"{base_url}?page={page_num}"
                
                print(f"üìÑ Page {page_num}: {url}")
                page.goto(url, timeout=30000)
                page.wait_for_timeout(2000)
                
                # Extraire tous les liens d'articles sur cette page
                articles_found = extract_article_links(page)
                
                if articles_found == 0:
                    print(f"‚ùå Aucun article trouv√© sur la page {page_num}, arr√™t")
                    break
                
                page_num += 1
                time.sleep(1)  # Pause entre les pages
                
            except Exception as e:
                print(f"‚ùå Erreur page {page_num}: {e}")
                break
        
        # 2. Section chroniques si elle existe
        print("\nüîç Scraping section chroniques...")
        chroniques_urls = [
            "https://www.lapresse.ca/debats/chroniques",
            "https://www.lapresse.ca/actualites/chroniques",
            "https://www.lapresse.ca/chroniques"
        ]
        
        for chronique_url in chroniques_urls:
            try:
                page.goto(chronique_url, timeout=30000)
                page.wait_for_timeout(2000)
                
                # Scroll pour charger plus de contenu
                for i in range(10):
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    page.wait_for_timeout(1000)
                
                extract_article_links(page, filter_lagace=True)
                
            except Exception as e:
                print(f"‚ùå Erreur section chroniques {chronique_url}: {e}")
        
        browser.close()

def extract_article_links(page, filter_lagace=False):
    """Extrait les liens d'articles depuis une page"""
    articles_found = 0
    
    # S√©lecteurs pour trouver les liens d'articles
    link_selectors = [
        "a[href*='/actualites/']",
        "a[href*='/debats/']",
        "a[href*='/chroniques/']",
        "a[href*='/sports/']",
        "a[href*='/arts/']",
        ".article-link",
        "[data-testid*='article'] a",
        "h1 a, h2 a, h3 a",
        ".headline a",
        ".title a"
    ]
    
    for selector in link_selectors:
        try:
            elements = page.locator(selector).all()
            for element in elements:
                href = element.get_attribute("href")
                if href and href not in scraped_urls:
                    if not href.startswith("http"):
                        href = "https://www.lapresse.ca" + href
                    
                    # Si on filtre, v√©rifier que c'est un article de Lagac√©
                    if filter_lagace and not ("lagace" in href.lower() or "patrick-lagace" in href.lower()):
                        continue
                    
                    scraped_urls.add(href)
                    articles_found += 1
                    print(f"üîó Article trouv√©: {href}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur avec s√©lecteur {selector}: {e}")
    
    return articles_found

def download_and_verify_articles():
    """T√©l√©charge et v√©rifie que chaque article est bien de Lagac√©"""
    if not scraped_urls:
        print("‚ùå Aucun article trouv√©.")
        return
    
    print(f"\nüì• V√©rification et t√©l√©chargement de {len(scraped_urls)} articles...")
    
    verified_articles = []
    
    for i, url in enumerate(scraped_urls, 1):
        print(f"üì• [{i}/{len(scraped_urls)}] V√©rification: {url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
            
            soup = BeautifulSoup(r.text, "html.parser")
            
            # V√©rifier si c'est vraiment un article de Lagac√©
            if not is_lagace_article(soup, url):
                print(f"‚ö†Ô∏è Pas un article de Lagac√©, ignor√©")
                continue
            
            # Extraction du titre
            titre = "Sans titre"
            title_selectors = ["h1", ".article-title", ".story-title", "[data-testid='article-title']"]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    titre = title_elem.get_text(strip=True)
                    break
            
            # Extraction de la date
            date = "0000-00-00"
            date_selectors = ["time", ".article-date", ".story-date", "[data-testid='article-date']"]
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_attr = date_elem.get("datetime") or date_elem.get_text(strip=True)
                    if date_attr:
                        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_attr)
                        if date_match:
                            date = date_match.group(1)
                            break
            
            # Extraction du contenu
            texte = extract_article_content(soup)
            
            if not texte:
                print(f"‚ö†Ô∏è Contenu vide, ignor√©")
                continue
            
            # Cr√©er une signature unique pour √©viter les doublons
            article_signature = f"{titre}_{date}_{len(texte)}"
            if article_signature in unique_articles:
                print(f"‚ö†Ô∏è Doublon d√©tect√©, ignor√©: {titre}")
                continue
            
            unique_articles.add(article_signature)
            
            # Cr√©ation du nom de fichier
            fname = f"{date}_Lagace_{clean_filename(titre)}.txt"
            filepath = OUTPUT_DIR / fname
            
            # Sauvegarde
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"TITRE : {titre}\n")
                f.write(f"URL: {url}\n")
                f.write(f"AUTEUR: Patrick Lagac√©\n")
                f.write(f"DATE: {date}\n\n")
                f.write(texte)
            
            verified_articles.append(fname)
            print(f"‚úÖ Sauvegard√© : {fname}")
            
            # Pause pour √©viter d'√™tre bloqu√©
            time.sleep(random.uniform(0.5, 1.5))
            
        except Exception as e:
            print(f"‚ùå Erreur pour {url}: {e}")
            continue
    
    return verified_articles

# Ex√©cution du script
if __name__ == "__main__":
    print("üöÄ Scraping cibl√© des articles de Patrick Lagac√©...")
    
    # √âtape 1: Scraping des archives
    print("\n" + "="*50)
    print("√âTAPE 1: Scraping des archives")
    print("="*50)
    scrape_archives()
    
    # √âtape 2: T√©l√©chargement et v√©rification
    print("\n" + "="*50)
    print("√âTAPE 2: T√©l√©chargement et v√©rification")
    print("="*50)
    verified_articles = download_and_verify_articles()
    
    print(f"\nüéâ Termin√© !")
    print(f"üìä {len(scraped_urls)} URLs trouv√©es")
    print(f"üìä {len(verified_articles)} articles de Lagac√© t√©l√©charg√©s")
    print(f"üìÅ Fichiers sauvegard√©s dans : {OUTPUT_DIR}")
    
    # Statistiques
    if verified_articles:
        dates = []
        for article in verified_articles:
            try:
                date_str = article.split("_")[0]
                if date_str != "0000-00-00":
                    dates.append(date_str)
            except:
                pass
        
        if dates:
            dates.sort()
            print(f"üìÖ Plus ancien: {dates[0]}")
            print(f"üìÖ Plus r√©cent: {dates[-1]}")